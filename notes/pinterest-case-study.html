<!doctype html>
<!-- AUTO-GENERATED from notes/pinterest-case-study.md - DO NOT EDIT DIRECTLY -->
<!-- Run 'python3 build.py' to regenerate this file -->
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>An Interesting Pinterest Case Study – Alice Chang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <!-- Open Graph / Social Sharing -->
    <meta property="og:title" content="An Interesting Pinterest Case Study" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://aljc.github.io/notes/pinterest-case-study.html" />
    <meta property="og:site_name" content="Alice Chang" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="An Interesting Pinterest Case Study" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <div class="site">
      <aside class="site-sidebar">
        <div class="site-sidebar-top">
          <div class="site-title-wrap">
            <a href="../about.html" style="text-decoration: none; color: inherit;">
              <div class="site-title">Alice Chang</div>
            </a>
          </div>
          <p class="site-tagline">
            Musings on technology, craft, and creativity.
          </p>
          <nav class="site-nav">
            <a href="../index.html" class="active">Notes</a>
            <div class="spine-list">
              <a href="thinking-with-machines.html">On Thinking With Machines</a>
              <a href="aesthetic-dna.html">Mapping Aesthetic DNA</a>
              <a href="pinterest-case-study.html">Pinterest Case Study</a>
            </div>
            <a href="../about.html">About</a>
          </nav>
        </div>
      </aside>

      <main class="site-main">
        <div class="section-label">Notes</div>
        
        <article class="essay-body">
          <div class="note-meta">July 11, 2024</div>

<h1>An Interesting Pinterest Case Study</h1>

<p><em>(The following is loosely adapted from a talk I gave at VentureBeat: Transform 2024.)</em></p>

<span class="dropcap">O</span>ne of the thorniest problems we encountered at Pinterest was a gap between language and taste. People often know what they like when they see it, but can't quite put the right words to it. Someone might have a board full of concrete, warm minimalism, but when asked to search for it, they just type something broad like "living room".

<img src="../assets/pinterest-board-example.png" alt="Pinterest Board Example" style="width: 100%; border-radius: 16px; margin: 32px auto; display: block;">

<h2>The Problem</h2>

<p>Consider a user with a home decor board that has a clear visual theme - clean lines, natural materials, exposed textures. They know they like it, but they don't know the terms (like "Japandi" or "industrial modern") to describe it.</p>

<p>Without the words, they can't articulate what they're looking for, and they can't search for more of it.</p>

<p>It was winter 2023 and GPT-3.5 had come out a few months prior (we were still pre GPT-4). I started wondering whether we could bridge this by combining Pinterest's visual search and taste graph with LLMs to generate these descriptors automatically, essentially giving users a language for their own aesthetic. I formed a small tiger team to explore the idea.</p>

<h2>Prompt to Product</h2>

<p>Our early prompting and prototype loop looked something like this:</p>

<ol>
<li> <strong>Sanity check</strong>: We asked an LLM to act as an interior designer and summarize a list of Pins. The first attempts were unexpectedly good and yielded not just formulaic keywords but rather a real sense of the underlying vibe.</li>
<li> <strong>Guardrails</strong>: We iterated on the early prompt several times, adding guardrails and a defined JSON schema so we could work with the output programmatically.</li>
<li> <strong>Prototype</strong>: We wrapped the JSON outputs in a lightweight UI and tested the flow on real user boards in a live sandbox environment.</li>
</ol>

<p>This loop allowed us to test and pivot the idea repeatedly in a matter of days, rather than weeks. (The final production buildout took approximately two additional months.)</p>

<h2>Product and Research</h2>

<p>Not only did this allow us to unlock unprecedented product development speed; we were also able to pilot a new operating model between product and research. We ran two parallel tracks:</p>

<ol>
<li> <strong>Product</strong>: Bootstrap with commercial LLMs to test and validate the user problem quickly.</li>
<li> <strong>Research</strong>: In parallel, invest in longer-horizon modeling innovations – in our case, <a href="https://www.pinterestcareers.com/media/eoqd5wcs/pinclip.pdf" target="_blank" rel="noopener noreferrer">PinCLIP</a> (Pinterest-specific foundational multimodal CLIP), <a href="https://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9" target="_blank" rel="noopener noreferrer">Pinterest Canvas</a> (text-to-image foundation model), and embedding work.</li>
</ol>

<p>Running these tracks in tandem let us to ship value and gather user data <em>while</em> deeper research was underway. The product scaffolding gave us a place to plug in new models as they matured, and the user data in turn shaped the research roadmap. It became a single loop rather than two disconnected efforts.</p>

<h2>Et voilà!</h2>

<video autoplay loop muted playsinline style="max-width: 300px; width: 30%; border-radius: 4px; float: left; margin: 0 24px 24px 0;">
  <source src="../assets/style-insights-demo.mp4" type="video/mp4">
<p>  Your browser does not support the video tag.</p>
</video>

<p>The final experience was an immersive, personalized style-insights page for every user. It quickly became a top-performing notification. But for me, the lasting takeaway was the working model: product and research moving at different speeds but along a shared loop. It’s an operating pattern I’ve continued to use for early-stage AI product discovery.</p>

<div style="clear: both;"></div>

        </article>
      </main>

      <div class="site-sidebar-bottom">
        <div class="footer-copyright">Alice Chang &copy; 2025</div>
        <div class="footer-quote">The medium is the message.</div>
      </div>
    </div>
    
    <script>
      // Highlight active spine link based on current URL
      document.addEventListener('DOMContentLoaded', () => {
        const currentPath = window.location.pathname.split('/').pop();
        const spineLinks = document.querySelectorAll('.spine-list a');
        spineLinks.forEach(link => {
          if (link.getAttribute('href') === currentPath) {
            link.classList.add('active');
          }
        });
      });
    </script>
  </body>
</html>

